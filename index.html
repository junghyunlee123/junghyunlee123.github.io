<!DOCTYPE HTML>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-JM0YYP897N"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-JM0YYP897N');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Lee, Jung Hyun</title>
  
  <meta name="author" content="Lee, Jung Hyun">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:1.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Lee, Jung Hyun</name>
              </p>
              <p>
	      	      I am a research scientist at NAVER Cloud in the Foundation Research team, working closely with <a style="text-decoration: none" target="_blank" href="https://scholar.google.com/citations?user=BqaWtH8AAAAJ&hl=en">Kang Min Yoo</a> and <a style="text-decoration: none" target="_blank" href="https://scholar.google.com/citations?hl=en&user=IUXo_qcAAAAJ">Jeonghoon Kim</a>. Before joining NAVER Cloud, I worked as a software engineer at <a style="text-decoration: none" target="_blank" href="https://research.samsung.com/">Samsung Research</a>.
                Prior to that, I completed my M.S. from <a style="text-decoration: none" target="_blank" href="https://gsai.kaist.ac.kr">the Graduate School of AI at KAIST</a>, where I was fortunately advised by <a style="text-decoration: none" target="_blank" href="https://mli.kaist.ac.kr/people">Eunho Yang</a>, and did my B.S. in mathematics from <a style="text-decoration: none" target="_blank" href="https://math.postech.ac.kr/en">POSTECH</a>.
              </p>

              <p style="text-align:center">
                onliwad101 at gmail dot com &nbsp/&nbsp
                <a target="_blank" href="data/CV_JHL.pdf">CV</a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com/citations?user=SVJcZosAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/onliwad101">GitHub</a> &nbsp/&nbsp
                <a target="_blank" href="https://www.linkedin.com/in/jung-hyun-lee-9a880b280">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/junghyunlee.JPG"><img style="width:70%;max-width:70%" alt="profile photo" src="images/junghyunlee.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Selected Papers (* equal contribution)</heading>
          
          <tr id="tvm">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/TVM.png' width="170" height="130">
              </div>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/pdf/2407.12863">
                <papertitle>Token-Supervised Value Models for Enhancing Mathematical Reasoning Capabilities of Large Language Models</papertitle>
              </a>
              <br />
              <strong>Jung Hyun Lee</strong>*, June Yong Yang*, Byeongho Heo, Dongyoon Han, Kang Min Yoo
              <br />
              <em>In preparation</em>
              <br />
              <a target="_blank" href="https://arxiv.org/pdf/2407.12863">arXiv</a>
              <p></p>
              <p>
                We presented <em>token-supervised value models (TVMs)</em>, new token-level verifiers trained to estimate the probability of reaching the correct final answer for each token in a solution.
              </p>
            </td>
          </tr>

          <tr onmouseout="lrq_stop()" onmouseover="lrq_start()" id="lrq">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lrq_image'>
                  <img src='images/LRQ1.png' width="170">
                </div>
                <img src='images/LRQ2.png' width="170" height="181">
              </div>
              <script type="text/javascript">
                function lrq_start() {
                  document.getElementById('lrq_image').style.opacity = "0";
                }

                function lrq_stop() {
                  document.getElementById('lrq_image').style.opacity = "1";
                }
                lrq_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/pdf/2306.00317">
                <papertitle>LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices</papertitle>
              </a>
              <br />
              <strong>Jung Hyun Lee</strong>*, Jeonghoon Kim*, June Yong Yang, Se Jung Kwon, Eunho Yang, Kang Min Yoo, Dongsoo Lee
              <br />
              <em>Under review</em>
              <br />
              <a target="_blank" href="https://arxiv.org/pdf/2407.11534">arXiv</a>
              /
	            <a target="_blank" href="https://github.com/onliwad101/FlexRound_LRQ">code</a>
              <p></p>
              <p>
                We proposed a new post-training weight quantization method for LLMs, <em>LRQ</em> that learns low-rank weight-scaling matrices instead of dense ones to decrease learnable parameters, thus enhancing the generalization capability of quantized LLMs. As a result, Llama and Llama 2 models can be quantized via LRQ with a minor impact on accauracy for commonsense reasoning tasks and MMLU.
              </p>
            </td>
          </tr>
          
          <tr id="peqa">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/PEQA1.png' width="170" height="105">
              </div>
            </td>
          <!-- <tr onmouseout="peqa_stop()" onmouseover="peqa_start()" bgcolor="#ffffd0" id="peqa">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='peqa_image'>
                  <img src='images/PEQA1.png' width="170">
                </div>
                <img src='images/PEQA2.png' width="170">
              </div>
              <script type="text/javascript">
                function peqa_start() {
                  document.getElementById('peqa_image').style.opacity = "1";
                }

                function peqa_stop() {
                  document.getElementById('peqa_image').style.opacity = "0";
                }
                peqa_stop()
              </script>
            </td> -->
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/pdf/2305.14152">
                <papertitle>Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization</papertitle>
              </a>
              <br />
              Jeonghoon Kim*, <strong>Jung Hyun Lee</strong>*, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo Lee
              <br />
              <em>Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2023
              <br />
              <a target="_blank" href="https://arxiv.org/pdf/2305.14152">arXiv</a>
              <p></p>
              <p>
                We introduced a quantization-aware parameter-efficient fine-tuning technique, <em>PEQA</em> that fine-tunes only the quantization step sizes of quantized LLMs to (i) reduce both the model size and the number of training parameters during fine-tuning, and (ii) accelerate inference latency after fine-tuning. Therefore, on a single NVIDIA A100-80GB GPU, Llama 65B can be fine-tuned via PEQA and then accelerated.
              </p>
            </td>
          </tr>
          
          <tr onmouseout="flexround_stop()" onmouseover="flexround_start()" id="flexround">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='flexround_image'>
                  <img src='images/FlexRound1.png' width="170">
                </div>
                <img src='images/FlexRound2.png' width="170">
              </div>
              <script type="text/javascript">
                function flexround_start() {
                  document.getElementById('flexround_image').style.opacity = "1";
                }

                function flexround_stop() {
                  document.getElementById('flexround_image').style.opacity = "0";
                }
                flexround_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/pdf/2306.00317">
                <papertitle>FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization</papertitle>
              </a>
              <br />
              <strong>Jung Hyun Lee</strong>*, Jeonghoon Kim*, Se Jung Kwon, Dongsoo Lee
              <br />
              <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, 2023
              <br />
              <a target="_blank" href="https://arxiv.org/pdf/2306.00317">arXiv</a>
              /
	            <a target="_blank" href="https://github.com/onliwad101/FlexRound_LRQ">code</a>
              <p></p>
              <p>
                We developed a new weight-rounding mechanism, <em>FlexRound</em> that can flexibly quantize pre-trained weights of not only computer vision models but also language models based on the magnitude of each weight. Specifically, Llama models can be quantized via FlexRound with marginal performance degradation on both commonsense reasoning tasks and causal language modeling.
              </p>
            </td>
          </tr>
          
          <!-- <tr id="cpq">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/CPQ-vertical.png' width="170" height="200">
              </div>
            </td> -->
          <tr onmouseout="cpq_stop()" onmouseover="cpq_start()" id="cpq">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cpq_image'>
                  <img src='images/CPQ1.png' width="170" height="139">
                </div>
                <img src='images/CPQ2.png' width="170">
              </div>
              <script type="text/javascript">
                function cpq_start() {
                  document.getElementById('cpq_image').style.opacity = "0";
                }

                function cpq_stop() {
                  document.getElementById('cpq_image').style.opacity = "1";
                }
                cpq_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/pdf/2109.02100">
                <papertitle>Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss</papertitle>
              </a>
              <br />
              <strong>Jung Hyun Lee</strong>*, Jihun Yun*, Sung Ju Hwang, Eunho Yang
              <br />
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br />
              <a target="_blank" href="https://arxiv.org/pdf/2109.02100">arXiv</a>
              <p></p>
              <p>
                We formulated <em>Cluster-Promoting Quantization (CPQ)</em>, which promotes cohesive clustering of full-precision weights around quantization grids. On top of that, we devised <em>DropBits</em> that drops bits rather than neurons/filters to lower the bias resulting from our multi-class straight-through estimator used in CPQ. We quantized ResNet-18 and MobileNetV2 on ImageNet with low-bit weight-activation quantization schemes.
              </p>
            </td>
          </tr>
          
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Services</heading>
                <p>Conference reviewer</p>
                <ul>
                  <li>AAAI Conference on Artificial Intelligence (AAAI): 2025</li>
                  <li>Association for Computational Linguistics (ACL) Rolling Review: 2024</li>
                  <li>International Conference on Machine Learning (ICML): 2024</li>
                  <li>International Conference on Learning Representations (ICLR): 2024, 2025</li>
                  <li>Neural Information Processing Systems (NeurIPS): 2022, 2023, 2024</li>
                </ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors & Awards</heading>
                <ul>
                  <li><strong>TOP 2</strong> in research, N INNOVATION AWARD 2023 (an internal excellence in technology awards ceremony hosted by NAVER)</li>
                </ul>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br />
              <p style="text-align:right;font-size:small;">
                Website template credit: <a target="_blank" href="https://jonbarron.info/" style="text-align:right;font-size:small;">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>

</body>

</html>

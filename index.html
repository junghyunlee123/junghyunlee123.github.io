<!DOCTYPE HTML>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-JM0YYP897N"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-JM0YYP897N');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Lee, Jung Hyun</title>
  
  <meta name="author" content="Lee, Jung Hyun">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:1.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Lee, Jung Hyun</name>
              </p>
              <p>
	      	      I am a research scientist at NAVER Cloud in the HyperCLOVA X team, working closely with <a style="text-decoration: none" target="_blank" href="https://scholar.google.com/citations?user=BqaWtH8AAAAJ&hl=en">Kang Min Yoo</a> and <a style="text-decoration: none" target="_blank" href="https://scholar.google.com/citations?hl=en&user=IUXo_qcAAAAJ">Jeonghoon Kim</a>. Before joining NAVER Cloud, I worked as a software engineer at <a style="text-decoration: none" target="_blank" href="https://research.samsung.com/">Samsung Research</a>.
                Prior to that, I completed my M.S. from <a style="text-decoration: none" target="_blank" href="https://gsai.kaist.ac.kr">the Graduate School of AI at KAIST</a>, where I was fortunately advised by <a style="text-decoration: none" target="_blank" href="https://mli.kaist.ac.kr/people">Eunho Yang</a>, and did my B.S. in mathematics from <a style="text-decoration: none" target="_blank" href="https://math.postech.ac.kr/en">POSTECH</a>.
              </p>

              <p style="text-align:center">
                onliwad101 at gmail dot com &nbsp/&nbsp
                <a target="_blank" href="data/CV_JHL.pdf">CV</a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com/citations?user=SVJcZosAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/onliwad101">GitHub</a> &nbsp/&nbsp
                <a target="_blank" href="https://www.linkedin.com/in/jung-hyun-lee-9a880b280">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/junghyunlee.JPG"><img style="width:70%;max-width:70%" alt="profile photo" src="images/junghyunlee.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Selected Papers (* equal contribution)</heading>
          
          <tr id="peqa">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/PEQA1.png' width="170">
              </div>
            </td>
          <!-- <tr onmouseout="peqa_stop()" onmouseover="peqa_start()" bgcolor="#ffffd0" id="peqa">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='peqa_image'>
                  <img src='images/PEQA1.png' width="170">
                </div>
                <img src='images/PEQA2.png' width="170">
              </div>
              <script type="text/javascript">
                function peqa_start() {
                  document.getElementById('peqa_image').style.opacity = "1";
                }

                function peqa_stop() {
                  document.getElementById('peqa_image').style.opacity = "0";
                }
                peqa_stop()
              </script>
            </td> -->
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/pdf/2305.14152">
                <papertitle>Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization</papertitle>
              </a>
              <br />
              Jeonghoon Kim*, <strong>Jung Hyun Lee</strong>*, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo Lee
              <br />
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2023
              <br />
              <a target="_blank" href="https://arxiv.org/pdf/2305.14152">arXiv</a>
              <p></p>
              <p>
                We introduced a parameter-efficient and quantization-aware adaption technique, PEQA that fine-tunes only quantization step sizes of a quantized large language model to reduce memory usage during fine-tuning and accelerate the inference latency. We applied PEQA to task-specific adaptation with Wikitext2 and PennTreeBank and instruction-tuning on Alpaca for Llama and Llama 2 models.
              </p>
            </td>
          </tr>
          
          <tr onmouseout="flexround_stop()" onmouseover="flexround_start()" bgcolor="#ffffd0" id="flexround">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='flexround_image'>
                  <img src='images/FlexRound1.png' width="170">
                </div>
                <img src='images/FlexRound2.png' width="170">
              </div>
              <script type="text/javascript">
                function flexround_start() {
                  document.getElementById('flexround_image').style.opacity = "1";
                }

                function flexround_stop() {
                  document.getElementById('flexround_image').style.opacity = "0";
                }
                flexround_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/pdf/2306.00317">
                <papertitle>FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization</papertitle>
              </a>
              <br />
              <strong>Jung Hyun Lee</strong>*, Jeonghoon Kim*, Se Jung Kwon, Dongsoo Lee
              <br />
              <em>International Conference on Machine Learning (ICML)</em>, 2023
              <br />
              <a target="_blank" href="https://arxiv.org/pdf/2306.00317">arXiv</a>
              /
	            <a target="_blank" href="https://github.com/onliwad101/FlexRound_LRQ">code</a>
              <p></p>
              <p>
                We proposed a new weight-rounding mechanism, <em>FlexRound</em>, which offers more flexible weight quantization based on the magnitude of pre-trained weights than previous PTQ methods. We quantized Llama models for both commonsense reasoning and causal language modeling tasks with both low-bit weight-only quantization and INT8 weight-activation quantization schemes.
              </p>
            </td>
          </tr>
          
          <!-- <tr id="cpq">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/CPQ-vertical.png' width="170" height="200">
              </div>
            </td> -->
          <tr onmouseout="cpq_stop()" onmouseover="cpq_start()" bgcolor="#ffffd0" id="cpq">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cpq_image'>
                  <img src='images/CPQ1.png' width="170" height="138">
                </div>
                <img src='images/CPQ2.png' width="170">
              </div>
              <script type="text/javascript">
                function cpq_start() {
                  document.getElementById('cpq_image').style.opacity = "0";
                }

                function cpq_stop() {
                  document.getElementById('cpq_image').style.opacity = "1";
                }
                cpq_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/pdf/2109.02100">
                <papertitle>Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss</papertitle>
              </a>
              <br />
              <strong>Jung Hyun Lee</strong>*, Jihun Yun*, Sung Ju Hwang, Eunho Yang
              <br />
              <em>International Conference on Computer Vision (ICCV)</em>, 2021
              <br />
              <a target="_blank" href="https://arxiv.org/pdf/2109.02100">arXiv</a>
              <p></p>
              <p>
                We formulated <em>Cluster-Promoting Quantization (CPQ)</em>, which promotes cohesive clustering of full-precision weights around quantization grids. In addition, we devised <em>DropBits</em> that drops bits rather than neurons/filters to lower the bias resulting from our multi-class straight-through estimator used in CPQ. We quantized ResNet-18 and MobileNetV2 on ImageNet with a low-bit weight-activation quantization.
              </p>
            </td>
          </tr>
          
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br />
              <p style="text-align:right;font-size:small;">
                Website template credit: <a target="_blank" href="https://jonbarron.info/" style="text-align:right;font-size:small;">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>

</body>

</html>
